{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yilong/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 21:36:10 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 21:36:11,109\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 21:36:14 config.py:542] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-03 21:36:14 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773', speculative_config=None, tokenizer='outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-03 21:36:15 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 03-03 21:36:15 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 03-03 21:36:15 model_runner.py:1110] Starting to load model outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 21:36:15 model_runner.py:1115] Loading model weights took 0.9277 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 21:36:16 worker.py:267] Memory profiling takes 0.45 seconds\n",
      "INFO 03-03 21:36:16 worker.py:267] the current vLLM instance can use total_gpu_memory (23.99GiB) x gpu_memory_utilization (0.90) = 21.59GiB\n",
      "INFO 03-03 21:36:16 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 19.14GiB.\n",
      "INFO 03-03 21:36:16 executor_base.py:110] # CUDA blocks: 104553, # CPU blocks: 21845\n",
      "INFO 03-03 21:36:16 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 51.05x\n",
      "INFO 03-03 21:36:16 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:07<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 21:36:23 model_runner.py:1562] Graph capturing finished in 7 secs, took 0.48 GiB\n",
      "INFO 03-03 21:36:23 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 8.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom/checkpoint-563\")\n",
    "llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, max_tokens=512, top_p = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_questions() -> Dataset:\n",
    "    data = load_dataset('parquet', data_files='data/cleaned_train.parquet')['train']\n",
    "    test_data = load_dataset('parquet', data_files='data/cleaned_test.parquet')['train']\n",
    "    return data , test_data\n",
    "\n",
    "\n",
    "dataset, testset = get_questions()\n",
    "#testset = get_gsm8k_questions('test')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer_pattern = r'<answer>(.*?)</answer>'\n",
    "    matches = list(re.finditer(answer_pattern, text, re.DOTALL))\n",
    "    if not matches:\n",
    "        return None\n",
    "        \n",
    "    final_answer = matches[-1].group(1).strip()\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    \"\"\"Normalizes the answer text for better comparison.\n",
    "    Args:\n",
    "        answer: Raw answer text\n",
    "    Returns:\n",
    "        Normalized answer text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    normalized = answer.lower()\n",
    "    # Remove extra whitespace\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "    # Remove punctuation that doesn't affect meaning\n",
    "    normalized = re.sub(r'[.,;:!?]', '', normalized)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(response, answer):\n",
    "    response_ = extract_xml_answer(response)\n",
    "    norm_response = normalize_answer(response_)\n",
    "    norm_answer = normalize_answer(answer)\n",
    "    if norm_response == norm_answer:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, testset = get_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-03 21:04:33 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  8.36it/s, est. speed input: 3183.49 toks/s, output: 209.07 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> In the toolbox within the center's premises. </think> <answer> toolbox </answer>\n",
      "plastic bin\n",
      "<think> In the high-rise embassy building, where the conference room is located. </think> <answer> high-rise embassy building </answer>\n",
      "conference room\n",
      "<think> In the consulate meeting room </think> <answer> consulate meeting room </answer>\n",
      "consulate meeting room\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "outputs= llm.chat([testset['prompt'][0], testset['prompt'][1], testset['prompt'][2]], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(testset['answer'][0])\n",
    "\n",
    "print(outputs[1].outputs[0].text)\n",
    "print(testset['answer'][1])\n",
    "\n",
    "print(outputs[2].outputs[0].text)\n",
    "print(testset['answer'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(llm, testset, n, n_repeat=5):\n",
    "    # Initialize result storage\n",
    "    all_results = []\n",
    "    if type(testset)==pd.core.frame.DataFrame:\n",
    "        testset.index = range(len(testset))\n",
    "    # Iterate through test set in chunks of n\n",
    "    with tqdm(range(0, len(testset['prompt']), n), desc=\"Processing batches\") as pbar:\n",
    "        for i in pbar:\n",
    "    #for i in tqdm(range(0, len(testset['prompt']), n), desc=\"Processing batches\"):\n",
    "            # Extract current batch of prompts and answers\n",
    "            batch_prompts = [p for p in testset['prompt'][i:i+n]]\n",
    "            batch_answers = [a for a in testset['answer'][i:i+n]]\n",
    "\n",
    "            # Get outputs from the LLM for the current batch\n",
    "            for k in range(n_repeat):\n",
    "                try:\n",
    "                    outputs = llm.chat(batch_prompts, \n",
    "                                    sampling_params=sampling_params, \n",
    "                                    use_tqdm=False)\n",
    "\n",
    "                    # Check correctness for the batch\n",
    "                    is_correct = [reward_func(each.outputs[0].text, answer) for each, answer in zip(outputs, batch_answers)]\n",
    "                    # Add detailed results for each item in the batch\n",
    "                    for j in range(len(batch_prompts)):\n",
    "                        result_entry = {\n",
    "                            \"prompt\": batch_prompts[j],\n",
    "                            \"answer\": batch_answers[j],\n",
    "                            \"output\": outputs[j].outputs[0].text,  # Assuming this is how to get the output\n",
    "                            \"correctness\": is_correct[j]\n",
    "                        }\n",
    "                        all_results.append(result_entry)\n",
    "                except:\n",
    "                    print(i, i+n)\n",
    "                    # for each in batch_prompts:\n",
    "                    #     print(each)\n",
    "                    #correct=[each['correctness'] for each in all_results]\n",
    "                    #print(f\"Correct: {np.mean(np.array(correct).flatten())}\")\n",
    "                    if k == n_repeat-1:\n",
    "                        return all_results\n",
    "                break\n",
    "            \n",
    "            if i%5 == 0:\n",
    "                correct=[each['correctness'] for each in all_results]\n",
    "                pbar.set_postfix(accuracy=np.mean(np.array(correct).flatten()))\n",
    "\n",
    "    # Save results to JSON file\n",
    "    #save_results(all_results)\n",
    "    correct=[each['correctness'] for each in all_results]\n",
    "    print(f\"Correct: {np.mean(np.array(correct).flatten())}\")\n",
    "    return all_results\n",
    "\n",
    "def save_results(results, filename=\"data/results/evaluation_tom.json\"):\n",
    "    \"\"\"Saves the evaluation results to a JSON file.\"\"\"\n",
    "    with open(filename, mode='w') as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**384 prompts model**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:54<00:00,  7.04it/s, accuracy=0.679]\n",
    "\n",
    "Correct: 0.6792152704135737\n",
    "\n",
    "**512 prompts model 773 check**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:47<00:00,  7.99it/s, accuracy=0.729]\n",
    "\n",
    "Correct: 0.7290899526564966\n",
    "\n",
    "**512 prompts model 300 check**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:51<00:00,  7.38it/s, accuracy=0.706]\n",
    "\n",
    "Correct: 0.7056962025316456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alll Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_all = load_dataset('parquet', data_files='data/test_data_all.parquet')['train']\n",
    "train_data_512 = load_dataset('parquet', data_files='data/cleaned_train_512.parquet')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "  'role': 'system'},\n",
       " {'content': \"Story: The consulate meeting room was a quiet, formal space with a large wooden table and leather chairs, surrounded by walls lined with the flags of different countries. Soft morning light filtered in through the tall windows, casting a warm glow on the metal file cabinet in the corner. Peyton pushed open the door, and the soft creak of the hinge broke the silence as she stepped into the consulate meeting room. The silver letter opener came to rest on the top of the metal file cabinet, its normally mundane presence now part of the carefully managed arrangement of objects in the consulate meeting room. A faint sound of metal meeting metal was the only sign of Peyton's deliberate reorganization. Joshua stepped across the threshold into the consulate meeting room, joined moments later by Mason, whose footsteps brought a new sense of purpose to the space. Together they stood amidst the flags, their presences filling the air with the promise of official works to come. The once-dull edge of the silver letter opener sparkled afresh, its new sharpness the result of Mason's thoughtful preparation as he worked to get everything just right.\\n Question:In which room does Joshua think that Peyton will search for the silver letter opener?\",\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_512['prompt'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**384 model**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:21<00:00,  6.54it/s, accuracy=0.632]\n",
    "\n",
    "Correct: 0.6317606444188723\n",
    "\n",
    "**512 token 773**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:10<00:00,  7.57it/s, accuracy=0.685]\n",
    "\n",
    "Correct: 0.6851990984222389\n",
    "\n",
    "**512 token 300**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:15<00:00,  7.02it/s, accuracy=0.66] \n",
    "\n",
    "Correct: 0.6595022624434389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  73%|███████▎  | 390/533 [00:55<00:15,  9.02it/s, accuracy=0.659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1940 1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  95%|█████████▌| 509/533 [01:12<00:04,  5.79it/s, accuracy=0.658]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2535 2540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 533/533 [01:15<00:00,  7.02it/s, accuracy=0.66] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.6595022624434389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_all = main_func(llm, test_data_all, 5, n_repeat=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 600 examples [00:00, 118846.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_faux = load_dataset('parquet', data_files='data/test/test_faux.parquet')['train']\n",
    "test_hitom = load_dataset('parquet', data_files='data/test/test_hitom.parquet')['train']\n",
    "test_tomi = load_dataset('parquet', data_files='data/test/test_tomi.parquet')['train']\n",
    "test_hitom_fixed = load_dataset('parquet', data_files='data/test/test_hitom_fixed.parquet')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['deception', 'story_length', 'question_order', 'sample_id', 'story', 'question', 'choices', 'answer', 'question_old', 'answer_old', 'prompt'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hitom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "  'role': 'system'},\n",
       " {'content': 'Story: Avery, Charlotte, Isabella, Elizabeth and Owen entered the living_room.\\nThe lettuce is in the green_drawer.\\nAvery moved the lettuce to the green_bathtub.\\nElizabeth dislikes the tangerine.\\nAvery exited the living_room.\\nCharlotte moved the lettuce to the blue_pantry.\\nOwen likes the green_envelope.\\nCharlotte exited the living_room.\\nIsabella made no movements and stayed in the living_room for 1 minute.\\nIsabella exited the living_room.\\nOwen dislikes the green_pepper.\\nElizabeth made no movements and stayed in the living_room for 1 minute.\\nElizabeth exited the living_room.\\nOwen moved the lettuce to the green_drawer.\\nOwen exited the living_room.\\nAvery, Charlotte, Isabella, Elizabeth and Owen entered the waiting_room.\\n Question:Where is the lettuce really?',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hitom['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 10.03it/s, est. speed input: 2370.28 toks/s, output: 250.73 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> The lettuce is in the green_drawer. </think> <answer> green_drawer </answer>\n",
      "green_drawer\n",
      "<think> Jacob lost his watch. </think> <answer> waiting_room </answer>\n",
      "blue_container\n",
      "<think> After the sequence of events, the eggplant is in the green_envelope. </think> <answer> green_envelope </answer>\n",
      "green_envelope\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs= llm.chat([test_hitom['prompt'][0], test_hitom['prompt'][1], test_hitom['prompt'][2]], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(test_hitom['answer'][0])\n",
    "\n",
    "print(outputs[1].outputs[0].text)\n",
    "print(test_hitom['answer'][1])\n",
    "\n",
    "print(outputs[2].outputs[0].text)\n",
    "print(test_hitom['answer'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 120/120 [00:18<00:00,  6.66it/s, accuracy=0.313]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.31333333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_hitom = main_func(llm, test_hitom, 5, n_repeat=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2/2 [00:00<00:00,  3.07it/s, accuracy=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_tomi = main_func(llm, test_tomi[90:100], 5, n_repeat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where was the broccoli at the beginning?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'box',\n",
       "  'output': '<think>Jackson and Chloe entered the hall. </think> <answer> dining_room </answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where will Mia look for the broccoli?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'bucket',\n",
       "  'output': '<think>In the bathtub. </think> <answer> dining_room </answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where does Mia think that Ethan searches for the broccoli?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'box',\n",
       "  'output': '<think>Jackson and Chloe are the two main characters in the story. Jackson enters the hall first, and Chloe enters the hall second. The boots are in the bathtub, so Jackson must have entered the hall before Chloe. Mia is the one who searches for the broccoli. </think> <answer> dining_room </answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where is the broccoli really?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'bucket',\n",
       "  'output': '<think>To find the actual location of the broccoli, we need to look at the story step by step.</think> \\n<answer>Chloe</answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where will Ethan look for the broccoli?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'box',\n",
       "  'output': '<think>In the dining_room</think> <answer>chopsticks</answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where does Ethan think that Mia searches for the broccoli?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'box',\n",
       "  'output': '<think> Jackson is the one who entered the hall and is searching for the boots. Chloe is the one who moved the boots to the pantry. </think> <answer> dining_room </answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where was the tie at the beginning?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'crate',\n",
       "  'output': '<think> At the beginning, the tie was in the bathtub. </think> <answer> bathtub </answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where will Lily look for the tie?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'bucket',\n",
       "  'output': '<think> Jackson is likely to look for the tie in the dining room. Chloe moved the boots to the pantry, so the tie is most likely inside the pantry. </think> <answer> dining room </answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where does Lily think that Abigail searches for the tie?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'bucket',\n",
       "  'output': '<think> Chloe entered the hall. </think> <answer> dining_room </answer>',\n",
       "  'correctness': 0},\n",
       " {'prompt': [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where is the tie really?',\n",
       "    'role': 'user'}],\n",
       "  'answer': 'bucket',\n",
       "  'output': '<think> In the bathtub. </think> <answer> the tie really is in the bathtub. </answer>',\n",
       "  'correctness': 0}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_tomi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': ['Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.',\n",
       "  'Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.'],\n",
       " 'question': ['Where was the broccoli at the beginning?',\n",
       "  'Where will Mia look for the broccoli?',\n",
       "  'Where does Mia think that Ethan searches for the broccoli?',\n",
       "  'Where is the broccoli really?',\n",
       "  'Where will Ethan look for the broccoli?',\n",
       "  'Where does Ethan think that Mia searches for the broccoli?',\n",
       "  'Where was the tie at the beginning?',\n",
       "  'Where will Lily look for the tie?',\n",
       "  'Where does Lily think that Abigail searches for the tie?',\n",
       "  'Where is the tie really?'],\n",
       " 'answer': ['box',\n",
       "  'bucket',\n",
       "  'box',\n",
       "  'bucket',\n",
       "  'box',\n",
       "  'box',\n",
       "  'crate',\n",
       "  'bucket',\n",
       "  'bucket',\n",
       "  'bucket'],\n",
       " 'question_type': ['true_belief',\n",
       "  'true_belief',\n",
       "  'true_belief',\n",
       "  'true_belief',\n",
       "  'true_belief',\n",
       "  'true_belief',\n",
       "  'false_belief',\n",
       "  'false_belief',\n",
       "  'false_belief',\n",
       "  'false_belief'],\n",
       " 'prompt': [[{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where was the broccoli at the beginning?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where will Mia look for the broccoli?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where does Mia think that Ethan searches for the broccoli?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where is the broccoli really?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where will Ethan look for the broccoli?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where does Ethan think that Mia searches for the broccoli?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where was the tie at the beginning?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where will Lily look for the tie?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where does Lily think that Abigail searches for the tie?',\n",
       "    'role': 'user'}],\n",
       "  [{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Story: Jackson entered the hall.\\nChloe entered the hall.\\nThe boots is in the bathtub.\\nJackson exited the hall.\\nJackson entered the dining_room.\\nChloe moved the boots to the pantry.\\n Question:Where is the tie really?',\n",
       "    'role': 'user'}]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tomi[90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
