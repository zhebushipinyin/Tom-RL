{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yilong/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 09:53:48 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 09:53:48,948\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 09:53:56 config.py:542] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 03-04 09:53:56 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773', speculative_config=None, tokenizer='outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-04 09:53:56 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 03-04 09:53:56 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 03-04 09:53:57 model_runner.py:1110] Starting to load model outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 09:53:57 model_runner.py:1115] Loading model weights took 0.9277 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 09:53:58 worker.py:267] Memory profiling takes 0.49 seconds\n",
      "INFO 03-04 09:53:58 worker.py:267] the current vLLM instance can use total_gpu_memory (23.99GiB) x gpu_memory_utilization (0.90) = 21.59GiB\n",
      "INFO 03-04 09:53:58 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 19.14GiB.\n",
      "INFO 03-04 09:53:58 executor_base.py:110] # CUDA blocks: 104553, # CPU blocks: 21845\n",
      "INFO 03-04 09:53:58 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 51.05x\n",
      "INFO 03-04 09:53:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:08<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 09:54:07 model_runner.py:1562] Graph capturing finished in 8 secs, took 0.68 GiB\n",
      "INFO 03-04 09:54:07 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 9.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom/checkpoint-563\")\n",
    "llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-300\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-WS/checkpoint-1268\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, max_tokens=512, top_p = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_questions() -> Dataset:\n",
    "    data = load_dataset('parquet', data_files='data/cleaned_train.parquet')['train']\n",
    "    test_data = load_dataset('parquet', data_files='data/cleaned_test.parquet')['train']\n",
    "    return data , test_data\n",
    "\n",
    "\n",
    "dataset, testset = get_questions()\n",
    "#testset = get_gsm8k_questions('test')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer_pattern = r'<answer>(.*?)</answer>'\n",
    "    matches = list(re.finditer(answer_pattern, text, re.DOTALL))\n",
    "    if not matches:\n",
    "        return None\n",
    "        \n",
    "    final_answer = matches[-1].group(1).strip()\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    \"\"\"Normalizes the answer text for better comparison.\n",
    "    Args:\n",
    "        answer: Raw answer text\n",
    "    Returns:\n",
    "        Normalized answer text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    normalized = answer.lower()\n",
    "    # Remove extra whitespace\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "    # Remove punctuation that doesn't affect meaning\n",
    "    normalized = re.sub(r'[.,;:!?]', '', normalized)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(response, answer):\n",
    "    response_ = extract_xml_answer(response)\n",
    "    norm_response = normalize_answer(response_)\n",
    "    norm_answer = normalize_answer(answer)\n",
    "    if norm_response == norm_answer:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, testset = get_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 09:54:13 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  8.58it/s, est. speed input: 3268.93 toks/s, output: 214.68 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> In the toolbox within the center's premises. </think> <answer> toolbox </answer>\n",
      "plastic bin\n",
      "<think> In the high-rise embassy building, where the conference room is located. </think> <answer> high-rise embassy building </answer>\n",
      "conference room\n",
      "<think> In the consulate meeting room </think> <answer> consulate meeting room </answer>\n",
      "consulate meeting room\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "outputs= llm.chat([testset['prompt'][0], testset['prompt'][1], testset['prompt'][2]], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(testset['answer'][0])\n",
    "\n",
    "print(outputs[1].outputs[0].text)\n",
    "print(testset['answer'][1])\n",
    "\n",
    "print(outputs[2].outputs[0].text)\n",
    "print(testset['answer'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(llm, testset, n, n_repeat=5):\n",
    "    # Initialize result storage\n",
    "    all_results = []\n",
    "    if type(testset)==pd.core.frame.DataFrame:\n",
    "        testset.index = range(len(testset))\n",
    "    # Iterate through test set in chunks of n\n",
    "    with tqdm(range(0, len(testset['prompt']), n), desc=\"Processing batches\") as pbar:\n",
    "        for i in pbar:\n",
    "    #for i in tqdm(range(0, len(testset['prompt']), n), desc=\"Processing batches\"):\n",
    "            # Extract current batch of prompts and answers\n",
    "            batch_prompts = [p for p in testset['prompt'][i:i+n]]\n",
    "            batch_answers = [a for a in testset['answer'][i:i+n]]\n",
    "\n",
    "            # Get outputs from the LLM for the current batch\n",
    "            for k in range(n_repeat):\n",
    "                try:\n",
    "                    outputs = llm.chat(batch_prompts, \n",
    "                                    sampling_params=sampling_params, \n",
    "                                    use_tqdm=False)\n",
    "\n",
    "                    # Check correctness for the batch\n",
    "                    is_correct = [reward_func(each.outputs[0].text, answer) for each, answer in zip(outputs, batch_answers)]\n",
    "                    # Add detailed results for each item in the batch\n",
    "                    for j in range(len(batch_prompts)):\n",
    "                        result_entry = {\n",
    "                            \"prompt\": batch_prompts[j],\n",
    "                            \"answer\": batch_answers[j],\n",
    "                            \"output\": outputs[j].outputs[0].text,  # Assuming this is how to get the output\n",
    "                            \"correctness\": is_correct[j]\n",
    "                        }\n",
    "                        all_results.append(result_entry)\n",
    "                except:\n",
    "                    print(k, i, i+n)\n",
    "                    # for each in batch_prompts:\n",
    "                    #     print(each)\n",
    "                    #correct=[each['correctness'] for each in all_results]\n",
    "                    #print(f\"Correct: {np.mean(np.array(correct).flatten())}\")\n",
    "                    if k == n_repeat-1:\n",
    "                        return all_results\n",
    "                break\n",
    "            \n",
    "            if i%5 == 0:\n",
    "                correct=[each['correctness'] for each in all_results]\n",
    "                pbar.set_postfix(accuracy=np.mean(np.array(correct).flatten()))\n",
    "\n",
    "    # Save results to JSON file\n",
    "    #save_results(all_results)\n",
    "    correct=[each['correctness'] for each in all_results]\n",
    "    print(f\"Correct: {np.mean(np.array(correct).flatten())}\")\n",
    "    return all_results\n",
    "\n",
    "def save_results(results, filename=\"data/results/evaluation_tom.json\"):\n",
    "    \"\"\"Saves the evaluation results to a JSON file.\"\"\"\n",
    "    with open(filename, mode='w') as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**384 prompts model**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:54<00:00,  7.04it/s, accuracy=0.679]\n",
    "\n",
    "Correct: 0.6792152704135737\n",
    "\n",
    "**512 prompts model 773 check**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:47<00:00,  7.99it/s, accuracy=0.729]\n",
    "\n",
    "Correct: 0.7290899526564966\n",
    "\n",
    "**512 prompts model 300 check**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:51<00:00,  7.38it/s, accuracy=0.706]\n",
    "\n",
    "Correct: 0.7056962025316456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alll Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_all = load_dataset('parquet', data_files='data/test_data_all.parquet')['train']\n",
    "train_data_512 = load_dataset('parquet', data_files='data/cleaned_train_512.parquet')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "  'role': 'system'},\n",
       " {'content': \"Story: The consulate meeting room was a quiet, formal space with a large wooden table and leather chairs, surrounded by walls lined with the flags of different countries. Soft morning light filtered in through the tall windows, casting a warm glow on the metal file cabinet in the corner. Peyton pushed open the door, and the soft creak of the hinge broke the silence as she stepped into the consulate meeting room. The silver letter opener came to rest on the top of the metal file cabinet, its normally mundane presence now part of the carefully managed arrangement of objects in the consulate meeting room. A faint sound of metal meeting metal was the only sign of Peyton's deliberate reorganization. Joshua stepped across the threshold into the consulate meeting room, joined moments later by Mason, whose footsteps brought a new sense of purpose to the space. Together they stood amidst the flags, their presences filling the air with the promise of official works to come. The once-dull edge of the silver letter opener sparkled afresh, its new sharpness the result of Mason's thoughtful preparation as he worked to get everything just right.\\n Question:In which room does Joshua think that Peyton will search for the silver letter opener?\",\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_512['prompt'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**384 model**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:21<00:00,  6.54it/s, accuracy=0.632]\n",
    "\n",
    "Correct: 0.6317606444188723\n",
    "\n",
    "**512 token 773**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:10<00:00,  7.57it/s, accuracy=0.685]\n",
    "\n",
    "Correct: 0.6851990984222389\n",
    "\n",
    "**512 token 300**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:15<00:00,  7.02it/s, accuracy=0.66] \n",
    "\n",
    "Correct: 0.6595022624434389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 311/533 [00:53<01:40,  2.22it/s, accuracy=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1545 1550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 533/533 [01:28<00:00,  6.00it/s, accuracy=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.6947685359427926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_all = main_func(llm, test_data_all, 5, n_repeat=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 176 examples [00:00, 19664.82 examples/s]\n",
      "Generating train split: 5994 examples [00:00, 792106.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_faux = load_dataset('parquet', data_files='data/test/test_faux.parquet')['train']\n",
    "test_hitom = load_dataset('parquet', data_files='data/test/test_hitom.parquet')['train']\n",
    "test_tomi = load_dataset('parquet', data_files='data/test/test_tomi.parquet')['train']\n",
    "test_hitom_fixed = load_dataset('parquet', data_files='data/test/test_hitom_fixed.parquet')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['deception', 'story_length', 'question_order', 'sample_id', 'story', 'question', 'choices', 'answer', 'question_old', 'answer_old', 'prompt'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hitom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "  'role': 'system'},\n",
       " {'content': 'Story: Charlotte, Isabella, Elizabeth and Owen entered the living_room. lettuce is in the green_drawer. moved the lettuce to the green_bathtub. dislikes the tangerine. exited the living_room. moved the lettuce to the blue_pantry. likes the green_envelope. exited the living_room. made no movements and stayed in the living_room for 1 minute. exited the living_room. dislikes the green_pepper. made no movements and stayed in the living_room for 1 minute. exited the living_room. moved the lettuce to the green_drawer. exited the living_room. Charlotte, Isabella, Elizabeth and Owen entered the waiting_room.\\n Question:Where is the lettuce really?',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hitom['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 11.10it/s, est. speed input: 2340.14 toks/s, output: 244.37 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> The lettuce is in the green_drawer. </think> <answer> green_drawer </answer>\n",
      "green_drawer\n",
      "<think> In the green_bottle </think> <answer> green_bottle </answer>\n",
      "blue_container\n",
      "<think>In the green_basket. </think> <answer> green_basket </answer>\n",
      "green_envelope\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs= llm.chat([test_hitom['prompt'][0], test_hitom['prompt'][1], test_hitom['prompt'][2]], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(test_hitom['answer'][0])\n",
    "\n",
    "print(outputs[1].outputs[0].text)\n",
    "print(test_hitom['answer'][1])\n",
    "\n",
    "print(outputs[2].outputs[0].text)\n",
    "print(test_hitom['answer'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 25/120 [00:04<00:37,  2.54it/s, accuracy=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▉      | 47/120 [00:10<00:30,  2.38it/s, accuracy=0.236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  76%|███████▌  | 91/120 [00:20<00:11,  2.54it/s, accuracy=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 120/120 [00:30<00:00,  3.90it/s, accuracy=0.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.24957264957264957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_hitom = main_func(llm, test_hitom, 5, n_repeat=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▌        | 180/1199 [00:30<09:48,  1.73it/s, accuracy=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▋       | 317/1199 [00:53<07:01,  2.09it/s, accuracy=0.396]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580 1585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  43%|████▎     | 510/1199 [01:24<04:37,  2.48it/s, accuracy=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2540 2545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 693/1199 [01:55<03:32,  2.38it/s, accuracy=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455 3460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|██████▋   | 798/1199 [02:13<03:30,  1.91it/s, accuracy=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3985 3990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  75%|███████▌  | 905/1199 [02:31<02:14,  2.18it/s, accuracy=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4515 4520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  78%|███████▊  | 931/1199 [02:36<01:50,  2.41it/s, accuracy=0.384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4645 4650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  87%|████████▋ | 1042/1199 [02:55<01:02,  2.52it/s, accuracy=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200 5205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1199/1199 [03:21<00:00,  5.94it/s, accuracy=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5990 5995\n",
      "Correct: 0.3912605042016807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_tomi = main_func(llm, test_tomi, 5, n_repeat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00, 13.52it/s, est. speed input: 2457.88 toks/s, output: 67.64 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<answer> No </answer>\n",
      "Yes\n",
      "\"Nothing\"\n",
      "I'm sorry about your story.\n",
      "Alice.\n",
      "Alice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs= llm.chat([test_faux['prompt'][0], test_faux['prompt'][1], test_faux['prompt'][2]], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(test_faux['answer'][0])\n",
    "\n",
    "print(outputs[1].outputs[0].text)\n",
    "print(test_faux['answer'][1])\n",
    "\n",
    "print(outputs[2].outputs[0].text)\n",
    "print(test_faux['answer'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  69%|██████▉   | 25/36 [00:03<00:01,  7.46it/s, accuracy=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 115 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  78%|███████▊  | 28/36 [00:04<00:01,  7.00it/s, accuracy=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 130 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 36/36 [00:05<00:00,  6.23it/s, accuracy=0.247]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.2469879518072289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_faux = main_func(llm, test_faux, 5, n_repeat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'NS_512_ckpt773'\n",
    "pd.DataFrame(results_hitom).to_csv('data/results/hitom_{}.csv'.format(model_name), index=False)\n",
    "pd.DataFrame(results_tomi).to_csv('data/results/tomi_{}.csv'.format(model_name), index=False)\n",
    "pd.DataFrame(results_faux).to_csv('data/results/faux_{}.csv'.format(model_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.5B Model \n",
    "## WS 384\n",
    "### HiTom\n",
    "Processing batches: 100%|██████████| 120/120 [00:26<00:00,  4.51it/s, accuracy=0.25] \n",
    "\n",
    "Correct: 0.24957264957264957\n",
    "\n",
    "### ToMi\n",
    "Processing batches: 100%|██████████| 1199/1199 [03:20<00:00,  5.97it/s, accuracy=0.409]\n",
    "\n",
    "Correct: 0.4088702928870293\n",
    "\n",
    "### Faux Pas\n",
    "Processing batches: 100%|██████████| 36/36 [00:05<00:00,  7.17it/s, accuracy=0.261]\n",
    "\n",
    "Correct: 0.2608695652173913\n",
    "\n",
    "## NS 512 773ckpt\n",
    "### HiTom\n",
    "Processing batches: 100%|██████████| 120/120 [00:30<00:00,  3.90it/s, accuracy=0.25] \n",
    "\n",
    "Correct: 0.24957264957264957\n",
    "\n",
    "### ToMi\n",
    "Processing batches: 100%|██████████| 1199/1199 [03:21<00:00,  5.94it/s, accuracy=0.391]\n",
    "\n",
    "Correct: 0.3912605042016807\n",
    "\n",
    "### Faux Pas\n",
    "Processing batches: 100%|██████████| 36/36 [00:05<00:00,  6.23it/s, accuracy=0.247]\n",
    "\n",
    "Correct: 0.2469879518072289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
