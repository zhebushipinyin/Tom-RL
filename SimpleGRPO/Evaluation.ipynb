{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yilong/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 14:02:47 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 14:02:47,225\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 14:02:51 config.py:542] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-05 14:02:51 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='outputs/Qwen-0.5B-GRPO-TomC-FT-WS/checkpoint-200', speculative_config=None, tokenizer='outputs/Qwen-0.5B-GRPO-TomC-FT-WS/checkpoint-200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=outputs/Qwen-0.5B-GRPO-TomC-FT-WS/checkpoint-200, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-05 14:02:52 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 03-05 14:02:52 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 03-05 14:02:52 model_runner.py:1110] Starting to load model outputs/Qwen-0.5B-GRPO-TomC-FT-WS/checkpoint-200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.81it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.76it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 14:02:53 model_runner.py:1115] Loading model weights took 0.9277 GB\n",
      "INFO 03-05 14:02:53 worker.py:267] Memory profiling takes 0.46 seconds\n",
      "INFO 03-05 14:02:53 worker.py:267] the current vLLM instance can use total_gpu_memory (23.99GiB) x gpu_memory_utilization (0.90) = 21.59GiB\n",
      "INFO 03-05 14:02:53 worker.py:267] model weights take 0.93GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 19.14GiB.\n",
      "INFO 03-05 14:02:53 executor_base.py:110] # CUDA blocks: 104553, # CPU blocks: 21845\n",
      "INFO 03-05 14:02:53 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 51.05x\n",
      "INFO 03-05 14:02:56 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:09<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 14:03:05 model_runner.py:1562] Graph capturing finished in 9 secs, took 0.00 GiB\n",
      "INFO 03-05 14:03:05 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 12.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom/checkpoint-563\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-773\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-512/checkpoint-300\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-WS/checkpoint-1268\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-WS/checkpoint-700\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-FT-WS/checkpoint-400\")\n",
    "#llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-Tom-KK/checkpoint-724\")\n",
    "llm = LLM(model=\"outputs/Qwen-0.5B-GRPO-TomC-FT-WS/checkpoint-200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, max_tokens=512, top_p = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_questions() -> Dataset:\n",
    "    data = load_dataset('parquet', data_files='data/cleaned_train.parquet')['train']\n",
    "    test_data = load_dataset('parquet', data_files='data/cleaned_test.parquet')['train']\n",
    "    return data , test_data\n",
    "\n",
    "\n",
    "dataset, testset = get_questions()\n",
    "#testset = get_gsm8k_questions('test')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer_pattern = r'<answer>(.*?)</answer>'\n",
    "    matches = list(re.finditer(answer_pattern, text, re.DOTALL))\n",
    "    if not matches:\n",
    "        return None\n",
    "        \n",
    "    final_answer = matches[-1].group(1).strip()\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    \"\"\"Normalizes the answer text for better comparison.\n",
    "    Args:\n",
    "        answer: Raw answer text\n",
    "    Returns:\n",
    "        Normalized answer text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    normalized = answer.lower()\n",
    "    # Remove extra whitespace\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "    # Remove punctuation that doesn't affect meaning\n",
    "    normalized = re.sub(r'[.,;:!?]', '', normalized)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(response, answer):\n",
    "    response_ = extract_xml_answer(response)\n",
    "    norm_response = normalize_answer(response_)\n",
    "    norm_answer = normalize_answer(answer)\n",
    "    if norm_response == norm_answer:\n",
    "        return 1\n",
    "    elif \"in the \" in norm_response:\n",
    "        if norm_response.split('in the ')[-1] == norm_answer:\n",
    "            return 1\n",
    "    elif \"\\\"\" in norm_response:\n",
    "        if norm_response.split(\"\\\"\")[-1].split(\"\\\"\")[0]== norm_answer:\n",
    "            return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, testset = get_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 14:03:10 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  7.37it/s, est. speed input: 2806.63 toks/s, output: 324.40 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> Reasoning process: Charlotte's presence is interrupted by Nicholas's footsteps, leaving her with a brief moment of distraction. She quickly realizes that she will need to search for the walkie-talkie. </think> <answer> Toolbox </answer>\n",
      "plastic bin\n",
      "<think> reasoning process: Paige and Emily are likely searching for the silver letter opener together in the conference room. </think> <answer> conference room </answer>\n",
      "conference room\n",
      "<think> reasoning process: Peyton's presence and actions in the consulate meeting room suggest that she is likely the one to search for the silver letter opener. </think> <answer> consulate meeting room </answer>\n",
      "consulate meeting room\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "outputs= llm.chat([testset['prompt'][0], testset['prompt'][1], testset['prompt'][2]], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(testset['answer'][0])\n",
    "\n",
    "print(outputs[1].outputs[0].text)\n",
    "print(testset['answer'][1])\n",
    "\n",
    "print(outputs[2].outputs[0].text)\n",
    "print(testset['answer'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(llm, testset, n, n_repeat=5):\n",
    "    # Initialize result storage\n",
    "    all_results = []\n",
    "    if type(testset)==pd.core.frame.DataFrame:\n",
    "        testset.index = range(len(testset))\n",
    "    # Iterate through test set in chunks of n\n",
    "    with tqdm(range(0, len(testset['prompt']), n), desc=\"Processing batches\") as pbar:\n",
    "        for i in pbar:\n",
    "    #for i in tqdm(range(0, len(testset['prompt']), n), desc=\"Processing batches\"):\n",
    "            # Extract current batch of prompts and answers\n",
    "            batch_prompts = [p for p in testset['prompt'][i:i+n]]\n",
    "            batch_answers = [a for a in testset['answer'][i:i+n]]\n",
    "\n",
    "            # Get outputs from the LLM for the current batch\n",
    "            for k in range(n_repeat):\n",
    "                try:\n",
    "                    outputs = llm.chat(batch_prompts, \n",
    "                                    sampling_params=sampling_params, \n",
    "                                    use_tqdm=False)\n",
    "\n",
    "                    # Check correctness for the batch\n",
    "                    is_correct = [reward_func(each.outputs[0].text, answer) for each, answer in zip(outputs, batch_answers)]\n",
    "                    # Add detailed results for each item in the batch\n",
    "                    for j in range(len(batch_prompts)):\n",
    "                        result_entry = {\n",
    "                            \"prompt\": batch_prompts[j],\n",
    "                            \"answer\": batch_answers[j],\n",
    "                            \"output\": outputs[j].outputs[0].text,  # Assuming this is how to get the output\n",
    "                            \"correctness\": is_correct[j]\n",
    "                        }\n",
    "                        all_results.append(result_entry)\n",
    "                except:\n",
    "                    print(k, i, i+n)\n",
    "                    # for each in batch_prompts:\n",
    "                    #     print(each)\n",
    "                    #correct=[each['correctness'] for each in all_results]\n",
    "                    #print(f\"Correct: {np.mean(np.array(correct).flatten())}\")\n",
    "                    if k == n_repeat-1:\n",
    "                        return all_results\n",
    "                break\n",
    "            \n",
    "            if i%5 == 0:\n",
    "                correct=[each['correctness'] for each in all_results]\n",
    "                pbar.set_postfix(accuracy=np.mean(np.array(correct).flatten()))\n",
    "\n",
    "    # Save results to JSON file\n",
    "    #save_results(all_results)\n",
    "    correct=[each['correctness'] for each in all_results]\n",
    "    print(f\"Correct: {np.mean(np.array(correct).flatten())}\")\n",
    "    return all_results\n",
    "\n",
    "def save_results(results, filename=\"data/results/evaluation_tom.json\"):\n",
    "    \"\"\"Saves the evaluation results to a JSON file.\"\"\"\n",
    "    with open(filename, mode='w') as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**384 prompts model**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:54<00:00,  7.04it/s, accuracy=0.679]\n",
    "\n",
    "Correct: 0.6792152704135737\n",
    "\n",
    "**512 prompts model 773 check**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:47<00:00,  7.99it/s, accuracy=0.729]\n",
    "\n",
    "Correct: 0.7290899526564966\n",
    "\n",
    "**512 prompts model 300 check**\n",
    "\n",
    "Processing batches: 100%|██████████| 381/381 [00:51<00:00,  7.38it/s, accuracy=0.706]\n",
    "\n",
    "Correct: 0.7056962025316456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alll Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_all = load_dataset('parquet', data_files='data/test_data_all.parquet')['train']\n",
    "test_half_ft = load_dataset('parquet', data_files='data/cleaned_train_ft_with_structure_384.parquet', split='train[50%:]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qprop=params', 'param=num_people', 'level_2', 'question', 'answer', 'qprop=nth_order', 'qprop=non_unique_mental_state', 'sprop=is_false_belief_story_1st', 'sprop=is_false_belief_story_1st_and_2nd', 'sprop=story_accuracy_1st_raw', 'sprop=story_accuracy_1st_infilled', 'sprop=global_idx', 'param=story_type', 'param=num_stories_total', 'param=max_sentences', 'param=num_moves', 'param=num_rooms', 'data_source', 'prompt', 'ability', 'reward_model', 'extra_info', 'story', 'story_type'],\n",
       "    num_rows: 2215\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_half_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**384 model**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:21<00:00,  6.54it/s, accuracy=0.632]\n",
    "\n",
    "Correct: 0.6317606444188723\n",
    "\n",
    "**512 token 773**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:10<00:00,  7.57it/s, accuracy=0.685]\n",
    "\n",
    "Correct: 0.6851990984222389\n",
    "\n",
    "**512 token 300**\n",
    "\n",
    "Processing batches: 100%|██████████| 533/533 [01:15<00:00,  7.02it/s, accuracy=0.66] \n",
    "\n",
    "Correct: 0.6595022624434389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 129/443 [00:30<02:20,  2.23it/s, accuracy=0.661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 635 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  54%|█████▍    | 240/443 [00:57<00:40,  4.96it/s, accuracy=0.65] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1190 1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 443/443 [01:43<00:00,  4.26it/s, accuracy=0.659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.6594104308390023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_all = main_func(llm, test_half_ft, 5, n_repeat=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>output</th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. The...</td>\n",
       "      <td>does not know about it</td>\n",
       "      <td>&lt;think&gt; reasoning process: Jasmine's comments ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. The...</td>\n",
       "      <td>does not know about it</td>\n",
       "      <td>&lt;think&gt; reasoning process: Jasmine and Emily a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt                  answer  \\\n",
       "0  [{'content': 'You are a helpful assistant. The...  does not know about it   \n",
       "1  [{'content': 'You are a helpful assistant. The...  does not know about it   \n",
       "\n",
       "                                              output  correctness  \n",
       "0  <think> reasoning process: Jasmine's comments ...            1  \n",
       "1  <think> reasoning process: Jasmine and Emily a...            1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfre = pd.DataFrame(results_all)\n",
    "dfre.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "no     0.0\n",
       "yes    1.0\n",
       "Name: correctness, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfre.loc[dfre.answer.isin(['yes', 'no'])].groupby(['answer']).correctness.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 94/533 [00:24<01:43,  4.26it/s, accuracy=0.654]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 465 470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|███       | 160/533 [00:40<02:15,  2.76it/s, accuracy=0.665]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 795 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 533/533 [02:16<00:00,  3.90it/s, accuracy=0.661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.6606334841628959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_all = main_func(llm, test_data_all, 5, n_repeat=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_faux = load_dataset('parquet', data_files='data/test/test_faux.parquet')['train']\n",
    "test_hitom = load_dataset('parquet', data_files='data/test/test_hitom.parquet')['train']\n",
    "test_tomi = load_dataset('parquet', data_files='data/test/test_tomi.parquet')['train']\n",
    "test_hitom_fixed = load_dataset('parquet', data_files='data/test/test_hitom_fixed.parquet')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['deception', 'story_length', 'question_order', 'sample_id', 'story', 'question', 'choices', 'answer', 'question_old', 'answer_old', 'prompt'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hitom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.',\n",
       "  'role': 'system'},\n",
       " {'content': 'Story: Charlotte, Isabella, Elizabeth and Owen entered the living_room. lettuce is in the green_drawer. moved the lettuce to the green_bathtub. dislikes the tangerine. exited the living_room. moved the lettuce to the blue_pantry. likes the green_envelope. exited the living_room. made no movements and stayed in the living_room for 1 minute. exited the living_room. dislikes the green_pepper. made no movements and stayed in the living_room for 1 minute. exited the living_room. moved the lettuce to the green_drawer. exited the living_room. Charlotte, Isabella, Elizabeth and Owen entered the waiting_room.\\n Question:Where is the lettuce really?',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hitom['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  8.69it/s, est. speed input: 1880.21 toks/s, output: 534.71 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> reasoning process: The question mentions moving the lettuce to the living room, green_drawer, green_bathtub, and blue_pantry. The story states that the lettuce was in the green_drawer, green_bathtub, and blue_pantry before being moved. </think>\n",
      "<answer> living room </answer>\n",
      "green_drawer\n",
      "<think> reasoning process: The story describes Carter, Elizabeth, Hannah, and Jacob entering and exiting the bedroom multiple times, with the tomato being in different containers. We need to determine where the tomato was really. </think>\n",
      "<answer> bedroom </answer>\n",
      "blue_container\n",
      "<think> reasoning process: The eggplant is in the green_envelope because it was initially in the pantry. It then moved to the green_basket, then to the green_bathtub, and finally to the green_envelope. </think>\n",
      "<answer> green_envelope </answer>\n",
      "green_envelope\n",
      "<think> reasoning: Emily, Avery, Evelyn, and Mila are in the front yard, moving the spinach to the green_bucket, then exiting the yard, and returning to the front yard. Emily, Avery, and Evelyn are in the waiting room. </think> <answer> front yard </answer>\n",
      "green_bucket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs= llm.chat([test_hitom['prompt'][0], test_hitom['prompt'][1], test_hitom['prompt'][2], test_hitom['prompt'][3]], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(test_hitom['answer'][0])\n",
    "\n",
    "print(outputs[1].outputs[0].text)\n",
    "print(test_hitom['answer'][1])\n",
    "\n",
    "print(outputs[2].outputs[0].text)\n",
    "print(test_hitom['answer'][2])\n",
    "\n",
    "print(outputs[3].outputs[0].text)\n",
    "print(test_hitom['answer'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 4/120 [00:02<01:35,  1.22it/s, accuracy=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 16/120 [00:07<01:01,  1.68it/s, accuracy=0.0429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 75 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  60%|██████    | 72/120 [00:25<00:36,  1.33it/s, accuracy=0.0638]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 355 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 120/120 [00:42<00:00,  2.81it/s, accuracy=0.0821]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.08205128205128205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_hitom = main_func(llm, test_hitom, 5, n_repeat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  57%|█████▊    | 46/80 [00:12<00:19,  1.74it/s, accuracy=0.28] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 225 230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  76%|███████▋  | 61/80 [00:16<00:10,  1.79it/s, accuracy=0.302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 300 305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 80/80 [00:21<00:00,  3.79it/s, accuracy=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.28717948717948716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_tomi = main_func(llm, test_tomi[:400], 5, n_repeat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s, est. speed input: 1085.18 toks/s, output: 361.71 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think> reasoning process: The story ends with Emma saying \"I'm sorry about your story\" and Alice immediately saying \"What do you mean? \" </think> <answer> yes </answer>\n",
      "Yes\n",
      "<think> reasoning process: Alice's response to Emma's apology suggests she felt it was inappropriate to express regret. </think> <answer> They should not have said \"I'm sorry about your story.\" </answer>\n",
      "I'm sorry about your story.\n",
      "<think> reasoning process: Emma and Alice were both participants in the story competition. Emma wanted to win, but Alice was the winner. Emma was away from school, so she didn't participate in the competition. Alice saw Emma and said she was sorry for her story. Since Alice was the winner, Emma cannot win by saying she was sorry. </think> <answer> Emma </answer>\n",
      "Alice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs= llm.chat([test_faux['prompt'][0], test_faux['prompt'][1], test_faux['prompt'][2]], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(test_faux['answer'][0])\n",
    "\n",
    "print(outputs[1].outputs[0].text)\n",
    "print(test_faux['answer'][1])\n",
    "\n",
    "print(outputs[2].outputs[0].text)\n",
    "print(test_faux['answer'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 21/36 [00:05<00:03,  4.41it/s, accuracy=0.22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 95 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 36/36 [00:08<00:00,  4.21it/s, accuracy=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0.2222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_faux = main_func(llm, test_faux, 5, n_repeat=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'NS_512_ckpt773'\n",
    "pd.DataFrame(results_hitom).to_csv('data/results/hitom_{}.csv'.format(model_name), index=False)\n",
    "pd.DataFrame(results_tomi).to_csv('data/results/tomi_{}.csv'.format(model_name), index=False)\n",
    "pd.DataFrame(results_faux).to_csv('data/results/faux_{}.csv'.format(model_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.5B Model \n",
    "## WS 384\n",
    "### HiTom\n",
    "Processing batches: 100%|██████████| 120/120 [00:26<00:00,  4.51it/s, accuracy=0.25] \n",
    "\n",
    "Correct: 0.24957264957264957\n",
    "\n",
    "### ToMi\n",
    "Processing batches: 100%|██████████| 1199/1199 [03:20<00:00,  5.97it/s, accuracy=0.409]\n",
    "\n",
    "Correct: 0.4088702928870293\n",
    "\n",
    "### Faux Pas\n",
    "Processing batches: 100%|██████████| 36/36 [00:05<00:00,  7.17it/s, accuracy=0.261]\n",
    "\n",
    "Correct: 0.2608695652173913\n",
    "\n",
    "## NS 512 773ckpt\n",
    "### HiTom\n",
    "Processing batches: 100%|██████████| 120/120 [00:30<00:00,  3.90it/s, accuracy=0.25] \n",
    "\n",
    "Correct: 0.24957264957264957\n",
    "\n",
    "### ToMi\n",
    "Processing batches: 100%|██████████| 1199/1199 [03:21<00:00,  5.94it/s, accuracy=0.391]\n",
    "\n",
    "Correct: 0.3912605042016807\n",
    "\n",
    "### Faux Pas\n",
    "Processing batches: 100%|██████████| 36/36 [00:05<00:00,  6.23it/s, accuracy=0.247]\n",
    "\n",
    "Correct: 0.2469879518072289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
